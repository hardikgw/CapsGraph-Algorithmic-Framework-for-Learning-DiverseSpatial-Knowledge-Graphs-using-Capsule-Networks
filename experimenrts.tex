\section{Experiments} 
In this section we look at two experiments of using CapsGraph for semi-supervised deep learning of knowledge graphs. The first experiment uses generated knowledge graph from patents text obtained from USPTO (United States Patent and Trademark Office) for text analysis and second experiment uses labelled data from FDA (Food and Drug Administration) for link analysis. Both data-sets are publicly available and code is available on Github.
\subsection{Semi-supervised learning for text data}
This dataset is patents data files that has various fields and lot of unstructured text. Goal is to implement MLT (More Like This) feature to search similar patents to a given text of patent summary. The problem we are trying to solve is to find documents ("patents") similar to a given document and not a specific document. One document may have multiple parts and one of the parts may contain similarity to another whole document. Such "partial match" should be given higher priority. We will create embedding from documents such that related documents are close to each other in embedding space. We create document-document graph such that they share edges. The relationship between documents is coming from graph (HIN). Sentence query of word query will not work here since it will not capture graph data. We can start First step is information extraction from unstructured text to create knowledge graph.

\subsubsection{Knowledge Graph from Text} 
Natural Language Processing (NLP) techniques have advanced a lot in last decade. Various language models have been proposed for estimation of word representations in vector space, such as Word2Vec\cite{mikolov2013efficient} and GloVe\cite{Pennington2014GloveGV}. Here we use ELMo (Embeddings from Language Models)\cite{Peters_2018}. First we extract named entities from text and create a linked graph of specific meta structure by extracting entities and relations in specific domain. Next step is to extract information about the entities by finding relation between the extracted entities and object. At document (patent) level it is less important to preserve each entity relation respective to the document, hence, it is possible use probability apply filter using attention model to keep only relevant information at document level. To focus only on relevant parts of the document, we create trigram embedding to create vectors for each trigram and send it to attention layer to determine weights. 
\par Our goal is to encode nodes

\subsubsection{CapsGraph embedding from knowledge graph}
Our data is composed of RDF Triples as described in GeoTeGra\cite{patel2018geotegra}. First, we generate 3xN matrix for each triple where each column vector represents embedding of triple element. We create multiple layers for each subgraph. In one document we can create subgraph of relevant sentences or paragraphs. Entities resulted from ELMo can be one layer. In addition to NER, we also processed the document using NLP where we find relationship between entities. There relationship can be "mentions" of entity type and create another layer.

\subsubsection{Document similarity evaluation}
In the task of finding similar document for a given document, the results are calculated based on ranking of scores produces by sum of score functions on triples. To evaluate the results we evaluate similarity between each triple of source document and top 5 resulting document.
